# ğŸ“š ClarifAI â€“ AI that Listens, Sees and Explains

ClarifAI is a real-time AI-powered educational assistant designed to support students through multimodal inputs and adaptive, emotion-aware responses. It can handle **text**, **voice**, and **image-based queries**, and responds using a **local LLM (Gemma 2B via Ollama)**. The system detects student emotions using webcam analysis and simplifies explanations based on real-time feedback, creating a responsive, classroom-friendly AI tutor.

---

## ğŸ”§ How It Works

### ğŸ”¹ Text Interaction
- Users type a question into the chat.
- Input is filtered for educational relevance and passed to the local LLM.
- AI generates a contextual, academic response with optional visual aid.

### ğŸ”¹ Voice Input
- Web Speech API captures user speech and transcribes it in real-time.
- Transcribed query is treated the same as a typed input.
- Responses are streamed live to the user interface.

### ğŸ”¹ Image Input
- Users upload an image (diagram, handwritten question, etc.).
- System uses **EasyOCR** for text extraction or **BLIP** for image captioning.
- Extracted or interpreted content is semantically matched and answered.

### ğŸ”¹ Emotion Detection
- Webcam captures frames in the background.
- OpenCV processes and detects facial expressions (happy, confused, sad, etc.).
- If negative emotion is detected, the assistant suggests a simplified explanation.

### ğŸ”¹ Visual Aid Generation
- If the AI response includes steps or structured data, the system triggers:
  - **Graphviz** to generate flowcharts.
  - **Matplotlib** for charts/graphs.

---

## ğŸ—‚ï¸ Project Structure

```
clarifai-edu-assistant/
â”‚
â”œâ”€â”€ app.py                      # Main Flask application with routing and SSE
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # Project overview
â”‚
â”œâ”€â”€ static/                     # Static assets (style.css, images, etc.)
â”‚   â””â”€â”€ style.css               # UI styling
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html              # Frontend HTML file
â”‚
â”œâ”€â”€ edu_ollama_assistant.py     # LLM interaction and query handling
â”œâ”€â”€ edu_voice_assistant.py      # Voice input processing (Web Speech API)
â”œâ”€â”€ edu_image_assistant.py      # OCR + captioning + image query processing
â”œâ”€â”€ visual_generator.py         # Flowchart/chart generation logic
```

---

## ğŸ’» Tech Stack

| Component          | Technology                            |
|--------------------|----------------------------------------|
| Backend            | Python (Flask)                         |
| Language Model     | Gemma 2B (via Ollama)                  |
| Voice Input        | Web Speech API                         |
| Image Processing   | EasyOCR, BLIP (Hugging Face)           |
| Emotion Detection  | OpenCV                                 |
| Visuals            | Graphviz, Matplotlib                   |
| Frontend           | HTML, CSS, JavaScript                  |

---

## ğŸš€ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/chakri9133/ClarifAI-AI-Assistant.git
cd ClarifAI-AI-Assistant
```

### 2. Install Python dependencies
```bash
pip install -r requirements.txt
```

### 3. Start Ollama and load the LLM
```bash
ollama run gemma:2b
```

### 4. Run the Flask web server
```bash
python app.py
```

Visit `http://localhost:5050` in your browser.

---

## ğŸ§ª Features Summary

- âœ… Text, voice, and image input handling  
- âœ… Local LLM integration for offline AI support  
- âœ… Live emotion detection and simplified response logic  
- âœ… Flowchart and chart generation based on response type  
- âœ… Educational keyword filtering for safe AI use  

---

## ğŸ§  Example Use Cases

- Ask academic questions like â€œExplain Newtonâ€™s Second Lawâ€ (text/voice).  
- Upload diagrams to get explanations (e.g., circuits, biology flowcharts).  
- Speak a complex question; if the system detects confusion, it offers a simpler version.  
- Get visual aids for process-based topics like food chains or life cycles.  

---

## ğŸ”— GitHub Repository

Project Source Code: [ClarifAI GitHub](https://github.com/chakri9133/ClarifAI-AI-Assistant)

---

## ğŸ“½ï¸ Demo Video

Watch the project demo video here:  
â–¶ï¸ [ClarifAI Demo â€“ Google Drive](https://drive.google.com/file/d/1O16NL2WnBiTnAWRbJlajOMuEaR2oP_RI/view?usp=sharing)

---

## ğŸ“„ Project Report

Download the complete project report (PDF):  
ğŸ“„ [ClarifAI Report â€“ Google Drive](https://drive.google.com/file/d/1bBNCu4Y28i3FNy5WUYfYzVZJDF2HxsBe/view?usp=sharing)

---

## ğŸ¤ Contributors

- [**Hasya**](https://github.com/Chavva-HasyaReddy) â€“ Voice input, image captioning, OCR, visual rendering  
- [**Chakri**](https://github.com/chakri9133) â€“ Backend logic, LLM integration, emotion detection  
- [**Hima Sree**](https://github.com/Himasree08) â€“ Frontend UI, voice + emotion display, visual rendering  

---

## ğŸ“„ License

This project is part of the Intel Unnati Internship and is licensed for academic use.
